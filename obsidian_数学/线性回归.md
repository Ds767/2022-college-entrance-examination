# 线性回归

## 回归分析

### 回归直线(regression line)

回归方程 $\hat y=\hat bx+\hat a$ 


##### 最小二乘法(method of least square)

使$Q(\alpha ,\beta )=\sum _{i=1}^n(y_i-\beta x_i-\alpha )^2$ 最小时$\alpha ,\beta$ 的值.(即残差平方和最小).即
$$
\hat b=\frac {\sum _{i=1}^n(x_i-\bar x)(y_i-\bar y)}{\sum _{i=1}^n(x_i-\bar x)^2}=\frac {\Sigma _{i=1}^nx_iy_i-n\bar x\bar y}{\Sigma _{i=1}^nx_i^2-n\bar x^2}
$$

$$
\hat a=\bar y-\hat b\bar x
$$



其中$\bar x=\frac {1}{n}\sum _{i=1}^nx_i$ ,$\bar y=\frac {1}{n}\sum _{i=1}^ny_i$ .

$(\bar x,\bar y)$ 为样本的中心点.回归直线过样本的中心点.

##### 线性回归模型

完整表达式 $\begin {cases}y=bx+a+e\\ E(e)=0,D(e)=\sigma ^2>0\end {cases}$ 

$a,b$ 为模型的未知参数,$x$ 为解释变量,$y$ 为预报变量.$e$ 为随机变量,即**随机误差(random error)**.

##### 残差

$\hat e_i=y_i-\hat y_i=y_i-\hat bx_i-\hat a,i=1,2,\ldots ,n.$ 

$\hat e_i$ 为相应于点$(x_i,y_i)$ 的**残差(residual)**.

可纵坐标为残差,做**残差图**.

### 回归效果

$$
R^2=1-\frac {\sum _{i=1}^n(y_i-\hat y_i)^2}{\sum _{i=1}^n(y_i-\bar y)^2}
$$

$R^2$越大,模型拟合效果越好.

在线性回归模型中,$R^2$ 表示解释变量对于预报变量变化的贡献率.常用的选择模型的指标之一.
$$
r=\frac {\Sigma _{i=1}^n(x_i-\bar x)(y_i-\bar y)}{\sqrt{\Sigma _{i=1}^n(x_i-\bar x)^2\Sigma _{i=1}^n(y_i-\bar y)^2}}
$$
数学必修3 p92

相关系数r衡量两变量线性关系强弱.

| $r\in$                          | 相关性     |
| ------------------------------- | ---------- |
| $[-1,-0.75]$                    | 强负相关性 |
| $[0.75,1]$                      | 强正相关性 |
| $(-0.75,-0.30]\cup [0.30,0.75)$ | 相关性一般 |
| $[-0.25,0.25]$                  | 相关性较弱 |

## 非线性回归

通过对数变换将指数关系变为线性关系.
